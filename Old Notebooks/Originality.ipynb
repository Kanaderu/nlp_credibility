{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'credco_webconf_study_1_study_1_project_1_2018_02_21t22_43_18_00_00_anon_nolink.csv',\n",
    "#    'credco_webconf_study_2_study_2_project_1_2018_02_21t22_44_07_00_00_anon_nolink.csv',\n",
    "#    'credco_webconf_study_3_study_3_project_1_2018_02_21t22_44_40_00_00_anon_nolink.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['credibilitycoalition-webconf-2018/data/credco_webconf_study_1_study_1_project_1_2018_02_21t22_43_18_00_00_anon_nolink.csv']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [os.path.join('credibilitycoalition-webconf-2018', 'data', file) for file in files]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for idx, row in enumerate(csv_reader):\n",
    "            if idx == 0:\n",
    "                labels.append(row)\n",
    "            if idx > 0:\n",
    "                data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are of different size but the largest one is a superset of all the others\n",
    "# so we'll use the labels array that is the largest in size\n",
    "label = labels[np.argmax([len(label) for label in labels])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function will extract a column of data given the index\n",
    "get_data_col = lambda data, idx: [col[idx] for col in data if idx < len(col)]\n",
    "\n",
    "# function will extract data columns given titles from the master label\n",
    "get_data_col_from_titles = lambda data, titles, label: [get_data_col(data, label.index(col)) for col in titles] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Report Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 report_title_label columns\n",
      "['report_title']\n",
      "There are 1 media_content_label columns\n",
      "['media_content']\n",
      "There are 1 media_urls_label columns\n",
      "['media_url']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "r = re.compile('report_title')\n",
    "report_title_labels = list(filter(r.match, label))\n",
    "print('There are {} report_title_label columns'.format(len(report_title_labels)))\n",
    "print(report_title_labels)\n",
    "\n",
    "r = re.compile('media_content')\n",
    "media_content_labels = list(filter(r.match, label))\n",
    "print('There are {} media_content_label columns'.format(len(media_content_labels)))\n",
    "print(media_content_labels)\n",
    "\n",
    "r = re.compile('media_url')\n",
    "media_urls_labels = list(filter(r.match, label))\n",
    "print('There are {} media_urls_label columns'.format(len(media_urls_labels)))\n",
    "print(media_urls_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 report_title rows\n",
      "There are 50 media_content rows\n",
      "There are 50 media_url rows\n"
     ]
    }
   ],
   "source": [
    "report_titles = get_data_col_from_titles(data, report_title_labels, label)[0]\n",
    "print('There are {} report_title rows'.format(len(report_titles)))\n",
    "\n",
    "media_content = get_data_col_from_titles(data, media_content_labels, label)[0]\n",
    "print('There are {} media_content rows'.format(len(media_content)))\n",
    "\n",
    "media_urls = get_data_col_from_titles(data, media_urls_labels, label)[0]\n",
    "print('There are {} media_url rows'.format(len(media_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 unique report_title rows\n",
      "There are 46 unique media_content rows\n",
      "There are 50 unique media_url rows\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique report_title rows'.format(len(set(report_titles))))\n",
    "print('There are {} unique media_content rows'.format(len(set(media_content))))\n",
    "print('There are {} unique media_url rows'.format(len(set(media_urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city's gung-ho approach to development has destroyed the area's natural ability to drain away hurricane floodwaters.\n"
     ]
    }
   ],
   "source": [
    "print(media_content[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"[CLS] \" + media_content[2] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(content)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'city', \"'\", 's', 'gun', '##g', '-', 'ho', 'approach', 'to', 'development', 'has', 'destroyed', 'the', 'area', \"'\", 's', 'natural', 'ability', 'to', 'drain', 'away', 'hurricane', 'flood', '##water', '##s', '.', '[SEP]']\n",
      "[CLS]           101\n",
      "the           1,996\n",
      "city          2,103\n",
      "'             1,005\n",
      "s             1,055\n",
      "gun           3,282\n",
      "##g           2,290\n",
      "-             1,011\n",
      "ho            7,570\n",
      "approach      3,921\n",
      "to            2,000\n",
      "development   2,458\n",
      "has           2,038\n",
      "destroyed     3,908\n",
      "the           1,996\n",
      "area          2,181\n",
      "'             1,005\n",
      "s             1,055\n",
      "natural       3,019\n",
      "ability       3,754\n",
      "to            2,000\n",
      "drain        12,475\n",
      "away          2,185\n",
      "hurricane     7,064\n",
      "flood         7,186\n",
      "##water       5,880\n",
      "##s           2,015\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1996,  2103,  1005,  1055,  3282,  2290,  1011,  7570,  3921,\n",
      "          2000,  2458,  2038,  3908,  1996,  2181,  1005,  1055,  3019,  3754,\n",
      "          2000, 12475,  2185,  7064,  7186,  5880,  2015,  1012,   102]])\n",
      "torch.Size([1, 29])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 29])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)\n",
    "print(tokens_tensor.size())\n",
    "\n",
    "print(segments_tensors)\n",
    "print(segments_tensors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29, 768])\n",
      "torch.Size([12, 1, 29, 768])\n",
      "torch.Size([12, 29, 768])\n",
      "torch.Size([29, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "print(encoded_layers[0].size())\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "print(token_embeddings.size())\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.size())\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 29 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS] [-0.31816742  0.05438105  0.19741799 ... -0.4546941   0.21156302\n",
      "  0.6953806 ]\n",
      "1 the [-0.60313344 -0.31244376  0.26959664 ... -0.24248172  0.02819252\n",
      " -1.0843672 ]\n",
      "2 city [-0.33740592 -0.24141443  0.24089587 ... -0.36435008  0.33033687\n",
      " -0.61303365]\n",
      "3 ' [-0.26656497 -0.970989    0.47505647 ... -1.7185882  -0.11934346\n",
      "  0.14042984]\n",
      "4 s [-0.4484266  -1.2585407  -0.1768176  ... -1.5263525   0.27928066\n",
      "  0.5566943 ]\n",
      "5 gun [-0.02599564 -0.8469635   1.0729356  ... -0.53957427  0.29509512\n",
      "  0.16516672]\n",
      "6 ##g [-0.22292222 -0.6756145   1.0609485  ...  0.6896161  -0.93699056\n",
      "  0.21629222]\n",
      "7 - [-0.23356818 -0.5371182   0.7503763  ...  0.60370904  0.6619676\n",
      "  0.3775569 ]\n",
      "8 ho [-0.3019129  -0.46668643  1.3552309  ... -0.10347248 -0.14968055\n",
      "  0.30068105]\n",
      "9 approach [-0.42976162 -0.4311239  -0.09484182 ... -0.47402418 -0.49557117\n",
      " -0.13514036]\n",
      "10 to [-0.31524038 -0.20892137 -0.03023395 ...  0.18992932  0.42011207\n",
      "  0.23474798]\n",
      "11 development [ 0.35490698  0.23821795  0.25481728 ... -0.63476336 -0.23152289\n",
      " -0.06344096]\n",
      "12 has [ 0.08656846 -0.17190719 -0.3211152  ... -0.8748503  -0.3767578\n",
      "  0.91189307]\n",
      "13 destroyed [-0.41229796 -0.04777831 -0.27676263 ... -0.96880865 -0.7162863\n",
      " -0.28863257]\n",
      "14 the [-0.67545813 -0.67488354  0.65277326 ...  0.05456926 -0.5634723\n",
      "  0.24117321]\n",
      "15 area [ 0.03543778 -0.06900014  0.44235379 ... -0.4296302  -0.25480306\n",
      "  0.30786598]\n",
      "16 ' [ 0.06081983 -0.90940374  0.3173206  ... -0.48104435  0.34934235\n",
      "  0.57881314]\n",
      "17 s [-0.10382789 -1.0851375   0.11341627 ... -0.4354174  -0.05199936\n",
      "  0.99396586]\n",
      "18 natural [ 0.04434166 -0.09607113  0.4711256  ...  0.04081167 -0.35920492\n",
      "  0.27927476]\n",
      "19 ability [-0.20721468 -0.40850177 -0.17604737 ...  0.08954959  0.24053411\n",
      "  0.20476611]\n",
      "20 to [ 0.02607724 -0.49305078 -0.20172125 ...  1.0662757   0.566623\n",
      "  0.9966166 ]\n",
      "21 drain [ 0.7028341  -0.25680423  0.5244203  ... -0.14449231 -0.6598242\n",
      "  0.04045922]\n",
      "22 away [-0.4520068  -0.3036804   0.1512216  ...  0.71962374 -0.10955127\n",
      "  0.7442202 ]\n",
      "23 hurricane [-0.3604209  -0.01440707  0.39774778 ... -0.5002092  -1.0911142\n",
      " -0.48916596]\n",
      "24 flood [ 0.03572661  0.9393362   0.22592235 ...  0.05807042 -0.32824206\n",
      " -0.9554973 ]\n",
      "25 ##water [-0.26788676  0.59440476  0.5810377  ... -0.21216582 -0.10793777\n",
      " -0.03444056]\n",
      "26 ##s [-0.19937393  0.12756556  0.8065877  ...  0.6845418  -0.11293606\n",
      "  0.27190745]\n",
      "27 . [ 0.8885725  -0.03341716 -0.35446927 ... -0.13923223 -0.06315181\n",
      "  0.02760798]\n",
      "28 [SEP] [ 0.71828055 -0.14980099 -0.27234814 ...  0.02239484 -0.01038082\n",
      " -0.00966344]\n"
     ]
    }
   ],
   "source": [
    "for i, (token_str, token_vec) in enumerate(zip(tokenized_text, token_vecs_cat)):\n",
    "    print(i, token_str, token_vec.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
